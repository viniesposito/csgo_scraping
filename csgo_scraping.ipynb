{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_match_urls(url):\n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    soup = soup.find_all('div', class_ = 'results-sublist')\n",
    "\n",
    "    url_collection = []\n",
    "\n",
    "    for day in soup:\n",
    "        matches = day.find_all('a', class_ = 'a-reset', href = True)\n",
    "\n",
    "        for match in matches:\n",
    "            match_url = match['href']\n",
    "            url_collection.append(match_url)\n",
    "\n",
    "    return list(set(url_collection))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stats_url_and_match_info(match_url):\n",
    "\n",
    "    url = f'https://www.hltv.org{match_url}'\n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    result = soup.find('div', class_ = 'small-padding stats-detailed-stats')\n",
    "    \n",
    "    if result == None:\n",
    "        result = 'ignore'\n",
    "    else:\n",
    "        result = result.find('a')['href']\n",
    "        \n",
    "    text = soup.find('div', class_='standard-box veto-box').find('div',class_='padding preformatted-text').get_text()\n",
    "\n",
    "    text = list(filter(None,text.split('\\n')))\n",
    "    \n",
    "    match_type = text[0]\n",
    "    \n",
    "    if len(text) == 1:\n",
    "        match_round = 'NA'\n",
    "    else:\n",
    "        match_round = text[1][2:]\n",
    "\n",
    "        if '.' in match_round:\n",
    "            match_round = match_round.split('.')[0]\n",
    "    \n",
    "    match_info = [match_type,match_round]\n",
    "    \n",
    "    return result, match_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_match_info(match_url):\n",
    "    \n",
    "    url = f'https://www.hltv.org{match_url}'\n",
    "    \n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    text = soup.find('div', class_='standard-box veto-box').find('div',class_='padding preformatted-text').get_text()\n",
    "\n",
    "    text = list(filter(None,text.split('\\n')))\n",
    "    \n",
    "    match_type = text[0]\n",
    "    \n",
    "    if len(text) == 1:\n",
    "        match_round = 'NA'\n",
    "    else:\n",
    "        match_round = text[1][2:]\n",
    "\n",
    "        if '.' in match_round:\n",
    "            match_round = match_round.split('.')[0]\n",
    "    \n",
    "    match_info = [match_type,match_round]\n",
    "    \n",
    "    return match_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stats_table(stats_url, match_info):\n",
    "    \n",
    "    url = f'https://www.hltv.org{stats_url}'\n",
    "    \n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    championship = soup.find('div',class_='menu-header').get_text()\n",
    "\n",
    "    teams = [None] * 2\n",
    "    scores = [None] * 2\n",
    "\n",
    "    match_info_box = soup.find('div', class_='match-info-box')\n",
    "    time = match_info_box.find('span').get_text()\n",
    "    teams[0] = match_info_box.find('div', class_='team-left').find('a', class_='block text-ellipsis').get_text()\n",
    "    scores[0] = match_info_box.find('div', class_='team-left').find('div').get_text()\n",
    "    teams[1] = match_info_box.find('div', class_='team-right').find('a', class_='block text-ellipsis').get_text()\n",
    "    scores[1] = match_info_box.find('div', class_='team-right').find('div').get_text()\n",
    "\n",
    "    init_row = [championship, time] + match_info + [teams[0], scores[0], teams[1], scores[1]]\n",
    "\n",
    "    stats_tables = soup.find_all('table', class_='stats-table')\n",
    "\n",
    "    stats = []\n",
    "\n",
    "    for i, table in enumerate(stats_tables):\n",
    "        trs = table.find('tbody').find_all('tr')\n",
    "\n",
    "        for tr in trs:\n",
    "            tds = tr.find_all('td')\n",
    "            row = init_row + [teams[i]]\n",
    "\n",
    "            for td in tds:\n",
    "                row.append(td.get_text())\n",
    "\n",
    "            stats.append(row)\n",
    "\n",
    "    df = pd.DataFrame(stats)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_stats_table(df):\n",
    "    df.iloc[:,1] = pd.to_datetime(df.iloc[:,1], format = '%Y-%m-%d %H:%M')\n",
    "    df.iloc[:,5] = pd.to_numeric(df.iloc[:,5])\n",
    "    df.iloc[:,7] = pd.to_numeric(df.iloc[:,7])\n",
    "    df.iloc[:,12] = pd.to_numeric(df.iloc[:,12])\n",
    "    df.iloc[:,13] = pd.to_numeric(df.iloc[:,13].str.strip('%'))\n",
    "    df.iloc[:,14] = pd.to_numeric(df.iloc[:,14])\n",
    "    df.iloc[:,15] = pd.to_numeric(df.iloc[:,15])\n",
    "    df.iloc[:,16] = pd.to_numeric(df.iloc[:,16])\n",
    "    df.iloc[:,17] = pd.to_numeric(df.iloc[:,17])\n",
    "\n",
    "    df.head()\n",
    "\n",
    "    idx = pd.IndexSlice\n",
    "\n",
    "    def correct_cols(df,col):\n",
    "        extract = df.iloc[:,col].str.extractall(r'(\\d{1,3})')\n",
    "\n",
    "        df[f'{col}1'] = pd.to_numeric(extract.loc[idx[:,0],:].reset_index().iloc[:,-1])\n",
    "        df[f'{col}2'] = pd.to_numeric(extract.loc[idx[:,1],:].reset_index().iloc[:,-1])\n",
    "        correct_df = df.drop(col,axis=1)\n",
    "\n",
    "        return correct_df\n",
    "\n",
    "    df_new = correct_cols(df,11)\n",
    "    df_new = correct_cols(df_new,10)\n",
    "\n",
    "    colnames = ['Championship','Date','Match Type','Round','Team 1', 'Team 1 Score','Team 2', 'Team 2 Score','Player for','Player',\n",
    "                'Deaths','KAST %','KD Diff','ADR','FK Diff','Rating','Assists','Flash Assists','Kills','Headshots']\n",
    "\n",
    "    df_new.columns = colnames\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data = pd.DataFrame()\n",
    "\n",
    "# for pg in range(3,7):\n",
    "    \n",
    "#     page = f'https://www.hltv.org/results?offset={pg}00&stars=1'\n",
    "    \n",
    "#     match_urls = get_match_urls(page)\n",
    "#     print(page)\n",
    "#     page_data = pd.DataFrame()\n",
    "\n",
    "#     for url in match_urls:\n",
    "#         stats_url = get_stats_url(url)\n",
    "#         if stats_url != 'ignore':\n",
    "#             stats_table = get_stats_table(stats_url)\n",
    "#             clean_df = clean_stats_table(stats_table)\n",
    "#             page_data = page_data.append(clean_df)\n",
    "    \n",
    "#     data = data.append(page_data)\n",
    "\n",
    "#     data.to_csv(f'csgo_scraped_data_page{pg}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_match_data(pages):\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for i, page in enumerate(pages):\n",
    "    \n",
    "        match_urls = get_match_urls(page)\n",
    "        \n",
    "        page_data = pd.DataFrame()\n",
    "\n",
    "        for url in match_urls:\n",
    "            \n",
    "            #match_info = get_match_info(url)\n",
    "            \n",
    "            stats_url, match_info = get_stats_url_and_match_info(url)\n",
    "            if stats_url != 'ignore':\n",
    "                stats_table = get_stats_table(stats_url,match_info)\n",
    "                clean_df = clean_stats_table(stats_table)\n",
    "                page_data = page_data.append(clean_df)\n",
    "\n",
    "        data = data.append(page_data)\n",
    "        \n",
    "        print(f'Page {i+1} was scraped successfully')\n",
    "        \n",
    "        return data\n",
    "\n",
    "        #data.to_csv(f'csgo_scraped_data_page{pg}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 was scraped successfully\n"
     ]
    }
   ],
   "source": [
    "data = get_match_data(['https://www.hltv.org/results?stars=1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
